%!TEX root = ../../thesis.tex

% https://sci-hub.ru/https://doi.org/10.1016/j.sapharm.2020.10.013
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8409170/
% https://sci-hub.ru/10.1109/TMI.2009.2013851
% https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0068449
% https://sci-hub.st/10.1007/s10462-020-09830-9

\chapter{Background}\label{c:background}
This section is dedicated to providing the theoretical background which was needed for the study. 
This section is structured as follows:\\
Fist and foremost, an introduction of the operating principle of a \mct\space will be given in \cref{s:b-microct}.
Secondly, \cref{s:guide-creation} will give an overview of how a guide is composed.
Thirdly, evaluation of the segmentation techniques and algorithms is given in \cref{s:b-segmentation-basics}.
Finally, \cref{s:b-mouse-anatomy} will give a short overview over relevant anatomical structures of mice.


\section{\mct}\label{s:b-microct}
Microscopic computed tomography (\mct) as a technology has been around since the 1980s and has since then emerged as the preclinical analogue of the clinical CT \cite{clarkAdvancesMicroCTImaging2021,ritmanCurrentStatusDevelopments2011}.
In comparison with clinical CT the \mct\space features high spacial resolution, which makes it irreplaceable for non-destructive small animal studies \cite{clarkAdvancesMicroCTImaging2021,ritmanCurrentStatusDevelopments2011,keklikoglouMicroCTBiologicalBiomedical2021}.


\subsection{Operating principle}\label{s:b-mct-operation}
\mct\space technology is based upon the third-generation or so called ``rotate-rotate'' geometry of CT scanners \cite{flohrCTSystems2013}.
The basic principle in clinical CT being that both the X-ray source and the detector rotate around the patient table as part of a gantry assembly at the same time \cite{flohrCTSystems2013}.
A visual representation of this concept can be seen in \cref{fig:3ct}.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/ct.png}}
	\caption{Third generation CT basic setup \cite{fdaDrawingCTFan2019}}\label{fig:3ct}
\end{figure}

\noindent
As can be seen in \cref{fig:3ct}, the animal or inanimate object is placed on a table between the X-ray source and the detector.
In clinical CT, the X-ray tube is designed to produce a fan shaped beam, which passes through the patient onto a concave detector array.
Whereas, in most \mct\space the X-ray source produces a cone or pyramid shaped beam, which passes through the patient onto a two-dimensional or flat detector \cite{babaComparisonFlatpanelDetector2002,clarkAdvancesMicroCTImaging2021}.
For reference see \cref{fig:mct}.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/mct.png}}
	\caption{\mct\space basic setup\cite{orhanMicrocomputedTomographyMicroCT2020}}\label{fig:mct}
\end{figure}

\noindent
\mct\space X-ray sources usually use a fixed tungsten anode and therefore have to operate at lower voltage and current when compared to the rotating anode of clinical CT.
They can only achieve between 20 and 100\acrshort{kv} of tube voltage and between 0.05 and 1mA.
\begin{equation}\label{eqn:intensity-proportionality}
	I \propto (kV)^{2}*(mA)
\end{equation}

\noindent
The number of X-ray photons depends on the tube current, and the energy of the photons depends on the tube voltage squared.
Furthermore, the intensity of the beam can be described as being proportional to the product of those two factors, see: \cref{eqn:intensity-proportionality}.
The flat panel detector usually contains a scintillator screen, which when excited by X-rays gives off visible light \cite{babaComparisonFlatpanelDetector2002,ritmanCurrentStatusDevelopments2011,clarkMicroCTRodentsStateart2014}.
This light is then detected with an image sensor, which can be either a \acrfull{ccd} or a \acrfull{cmos} image sensor \cite{babaComparisonFlatpanelDetector2002}.
\mct\space utilize high resolution flat panel detectors with pixel sizes less than 100 $\mu m^{2}$ \cite{clarkAdvancesMicroCTImaging2021,clarkMicroCTRodentsStateart2014}.
They can however, achieve \gls{iso} \gls{vx} sizes between 400 nm and 70 $\mu m$ \cite{orhanMicrocomputedTomographyMicroCT2020}.
To produce a scan the \mct\space captures several hundred, two dimension cone beam projections of different angles around the animal or object \cite{clarkMicroCTRodentsStateart2014,ritmanCurrentStatusDevelopments2011,clarkAdvancesMicroCTImaging2021}.
The rotation angles can be in 180$^{\circ}$ or 360$^{\circ}$ \cite{orhanMicrocomputedTomographyMicroCT2020}.
Image information can be described as the amount of attenuation of the X-ray beam at a specific path through the volume \cite{,orhanMicrocomputedTomographyMicroCT2020}.
This attenuation is described by the following \cref{eqn:attenuation}:
\begin{equation}\label{eqn:attenuation}
	I_{x} = I_{0}e^{- \mu x}
\end{equation}

\noindent
Where $I_{0}$ is the initial intensity of the beam, $x$ is the distance the beam traveled and $\mu$ is the linear \acrlong{ac}.
The linear \acrfull{ac} is different for every tissue and material and describes its X-ray density.
The result of \cref{eqn:attenuation} is $I_{x}$, the intensity of the beam at a distance $x$ from the source \cite{liMicrocomputedTomographySmall2008,orhanMicrocomputedTomographyMicroCT2020}.
This is performed simultaneously for all detector pixels in a single projection.
If the process is repeated for multiple angles in small steps (see: \cref{fig:ct-projections}) the attenuation value for every \gls{vx} in the scan volume can be found.
Generally speaking, increasing the amount of projections leads to a more accurate result \cite{liMicrocomputedTomographySmall2008,orhanMicrocomputedTomographyMicroCT2020}.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/ct-projections.png}}
	\caption{\mct\space projections and reconstruction \cite{orhanMicrocomputedTomographyMicroCT2020}}\label{fig:ct-projections}
\end{figure}


\subsection{Data processing}\label{s:b-mct-processing}
Once all needed projections, as shown in \cref{fig:ct-projections}, are performed and stored on a computer, they can be reconstructed into the final image, or in the case of a volume dataset multiple images \cite{liMicrocomputedTomographySmall2008,orhanMicrocomputedTomographyMicroCT2020}.
A single projection can be described as a radiograph, which represents the intensity of the object at a particular angle \cite{liMicrocomputedTomographySmall2008,orhanMicrocomputedTomographyMicroCT2020}.
The radiographs differentiate either 65,536 (16-\gls{bit}) grey values or 256 (8-\gls{bit}) grey values, which are directly related to the X-ray absorption \cite{liMicrocomputedTomographySmall2008,orhanMicrocomputedTomographyMicroCT2020}.
To convert these two-dimensional radiographs into a three-dimensional dataset, the application of a reconstruction algorithm is required.
Multiple reconstruction methods exist, such as fourier or fast fourier, filtered back projection and iterative methods \cite{liMicrocomputedTomographySmall2008,orhanMicrocomputedTomographyMicroCT2020}.
Before any reconstruction algorithm can be utilized the data needs to be preprocessed.
This step compensates for factors such as detector response heterogeneity, defective pixels, X-ray intensity drift and electronic noise \cite{liMicrocomputedTomographySmall2008}.
Next, the reconstruction algorithm can be applied.
As mentioned above, there exist a multitude of reconstructed algorithms, however,
the most widely used algorithm in \mct\space is the \acrfull{fbp}.
Specifically, the \acrfull{fdk}, which is well suited for \mct\space because of its simplicity
and ability to handle data truncation in the z-axis or longitudinal direction \cite{liMicrocomputedTomographySmall2008,orhanMicrocomputedTomographyMicroCT2020}.
Building upon these simple algorithms, are iterative algorithm like the \gls{isra} and \gls{isratv},
which were utilized to reconstruct the datasets used in this study (see: \cref{tab:scan-parameters}).
As can be seen in \cref{fig:isra}, this algorithm starts of by performing a \acrshort{fbp} (bottom right \cref{fig:isra}) and storing the result.
Next, the current image is forward and back-projected to get an estimation image,
which then serves to calculate the error ratio between the estimation and the measured data.
The error is then considered in the next iteration of forward and back-projection to update the estimate image.
In case of the \gls{isratv}, which is used to improve reconstruction results for datasets with a small number of projections,
\acrfull{tv} enforces a sparse image gradient.
Effectively, this modification of the \gls{isra} algorithm, enforces uniformity between edges \cite{vandeghinsteLowDoseMicroCTImaging2013}.
\cref{fig:isra} shows \acrshort{tv} being used to enhance the \gls{isra} method by
calculating the \glsxtrshort{tv} on the previous back-projected image and modifying the error ratio for the next iteration cycle \cite{vandeghinsteLowDoseMicroCTImaging2013}.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/isra.png}}
	\caption{\gls{isra} and \gls{isratv} iterative algorithm principle \cite{vandeghinsteLowDoseMicroCTImaging2013}}\label{fig:isra}
\end{figure}

\noindent
\cref{fig:alg-comp} shows a comparison of reconstruction results between \gls{fdk}, \gls{isra} and \gls{isratv}
in relation to the number of projection views.
When comparing the algorithms results with the lowest number (32) of projection views,
it can be clearly seen that \gls{fdk} suffers the most from streak artifacts and image noise.
\gls{isra} is able to correct for most streak artifacts, but still struggles with image noise.
Finally, \gls{isratv} is able to correct for streak artifacts and image noise \cite{vandeghinsteLowDoseMicroCTImaging2013}.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/alg-comp.png}}
	\caption{\gls{fdk}, \gls{isra} and \gls{isratv} reconstruction quality comparison \cite{vandeghinsteLowDoseMicroCTImaging2013}}\label{fig:alg-comp}
\end{figure}


\subsection{\acrfull{hu}}\label{s:b-hu}
\acrfull{hu} are directly related to the \acrfull{ac} $\mu$
and can also be called the CT number.
\begin{equation}
	\acrshort{hu} = \frac{\mu - \mu_{water}}{\mu_{water}}*1000
\end{equation}\label{eqn:hu}

\noindent
\cref{eqn:hu} shows how \acrshort{hu} are computed.
Where, $\mu$ is the \acrlong{ac} of a specific tissue or material and $\mu_{water}$
is the \acrlong{ac} of water.
It is calculated as the difference of the \acrlong{ac} of a tissue and water,
divided by the \acrshort{ac} of water and finally multiplied by the factor $1000$ \cite{liMicrocomputedTomographySmall2008}.
Effectively, all \acrshort{hu} values are calculated as a comparison to the \acrlong{ac} of water.
This results in some notable \acrshort{hu} values on the Houndsfield scale.
The most notable one being water, which is zero by definition of \cref{eqn:hu}.
Air, which has an \acrlong{ac} close to zero, has the \acrshort{hu} value of $-1000$.
Bone tissue, which has about twice the \acrlong{ac} of water, has a CT number of $+1000$.
While most soft tissues have a \acrshort{hu} value between $-100$ and $+100$ \cite{liMicrocomputedTomographySmall2008}.
\Cref{fig:hu-scale} gives an overview over the most common clinical \acrshort{hu} values.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/huScale.png}}
	\caption{\acrshort{hu} for tissues in clinical setting \cite{jungBasicPhysicalPrinciples2021}}\label{fig:hu-scale}
\end{figure}


\pagebreak
\section{Composition of a guide}\label{s:guide-creation}
%TODO: write something


\subsection{\good}\label{s:b-gooddocs}
\begin{displayquote}[\cite{shorterGoodDocsProject2024}]
	``The Good Docs Project educates and empowers people to create high-quality documentation by providing them with templates, resources, best practices, and tools to enhance their documentation in open source and beyond.''
\end{displayquote}
It is used for documentation design by companies like GitLab (GitLab Inc., San Francisco, USA) and JetBrains (JetBrains s.r.o, Prague, Czech Republic).
\good\space actively discourages producing any sort of documentation before its content and structure have been planned extensively \cite{shorterGoodDocsProject2024}.
Instead, it instructs documentation authors to first and foremost, think about the topic at hand.
Especially, authors should evaluate the complexity of the topic and try to predict what kind of questions readers might have while reading the documentation.
Furthermore, documentation authors are supposed to consider the ability of their target audience and what reasonable assumptions one can make about the prior knowledge of
readers.
Next, \good\space instructs documentation authors to create three fictional readers: a beginner, an intermediate and an expert.
The beginner might represent an ordinary customer or user of some software or technology product.
The intermediate might represent a system administrator of said product
and the expert might represent a support person.
The documentation author should now, come up with several problems each fictional reader would try to solve with the help of documentation \cite{shorterGoodDocsProject2024}.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/gd_dt.png}}
	\caption{\good\space critical paths decision table \cite{shorterGoodDocsProject2024}}\label{fig:dg-dt}
\end{figure}

\noindent
Furthermore, the author is supposed to compose a table similar to \cref{fig:dg-dt} and score the likeliness each reader is
going to use the documentation to complete the task.
The score ranges from 1, which is the lowest likelihood to 3 for the highest likelihood.
Next, the author needs to sum up each row.
The resulting ``Critical Path Score'' is then intended to give the author insight about the importance of different documentation
content \cite{shorterGoodDocsProject2024}.
``Critical Path Score'' ranges from 3\footnote{3 readers, each with a likelihood of 1. $3*1=3$},
which is the lowest importance to the highest importance of 9\footnote{3 readers, each with a likelihood of 3. $3*3=9$}.
Finally, based on all information gathered in the process explained above, the author is instructed to choose the appropriate template provided by \good\space \cite{shorterGoodDocsProject2024}.\\
The following compressed list provides an overview over the available templates in the current edition of \good\space version 1.2.0:
\begin{description}
	\item[Concept] An explanation of a concept, context, or background information about a product or its features.
	\item[How-to] A concise set of numbered steps to do one task with the product.
	\item[README] Information users need to know about your project, including how users can engage with the project and get started with the tool.
	\item[Reference] Specific, in-depth details about a particular topic.
	\item[Release notes] Communicate new features, improvements, bug fixes, and known issues about a product to users and stakeholders.
	\item[Tutorial] Instructions for setting up an example project using the product, intended for the purpose of hands-on learning.
	\item[Troubleshooting] A list of common problems (referred to as ``symptoms'') experienced by users, an explanation of the causes, and steps to resolve the issue.
	\item[Bug report] The bug report is an issue template that your users can fill out to provide you with higher-quality, actionable bug issues for your product.
	\item[Code of Conduct] A Code of Conduct helps you govern your open source or developer community to ensure it remains healthy and open.
	\item[Contributing guide] A CONTRIBUTING document tells users how they can contribute to your open source project and join the community.
	\item[Our team] Helps you clearly communicate who belongs to your open source project or organization and how contributors can contact or work with them.
	\item[API quickstart] API quickstarts describe the easiest way for readers to achieve a result that shows off the capabilities of the service.
	\item[API reference] API references are technical manuals that provide API specifications and integration instructions to help your audience understand the service.
	\item[Contact support] A contact support page typically includes a list of the communication channels, discussion forums, and links to other resources to assist users with issues that they are having with your product.
	\item[Glossary] A reference document that lists and organizes terms and their definitions that are unique to your organization or which you use in a specific way.
	\item[Installation guide] Explain all the necessary steps to install the product and set it up for further use.
	\item[Quickstart] A quickstart introduces your users to your application for the first time. It focuses on the primary feature of the application and helps your users to start using the application as quickly as possible.
\end{description}


\subsection{\diatx}\label{s:diataxis}
The ``\diatx\space documentation framework'' by Daniele Procida \cite{procidaDiataxisDocumentationFramework2023,aghajaniSoftwareDocumentationPractitioners2020}
is extensively used for technical documentation by software projects like NumPy \cite{harrisArrayProgrammingNumPy2020}, Django \cite{Django2013} and large companies like Canonical (Canonical Limited, Douglas, Isle of Man) and Cloudflare (Cloudflare, Inc., San Francisco, California, U.S.A.).
\diatx\space bases itself upon four basic concepts to divide documentation into four different types with different structures according to their goals.\\
\begin{figure}
	\begin{center}
		\begin{tikzpicture}
			\draw [<-,thick,align=center] (-6, 0) -- (0,0) node[midway,above] (acq) {Acquisition (Serves study)};
			\draw [->,thick,align=center] (0, 0) -- (6,0) node[midway,above] (app) {Application (Serves work)};
			\draw [<-,thick,align=center] (0, -7) -- (0,0) node[midway,below,sloped] (cog) {Cognition (Theoretical knowledge)};
			\draw [->,thick,align=center] (0, 0) -- (0,7) node[midway,below,sloped] (act) {Action (Practical steps)};
			\node[below = 2cm of app] (ref) {Reference};
			\node[orange, above = 0.1cm of ref] {Information-oriented};
			\node[below = 2cm of acq] (ex) {Explanation};
			\node[orange, above = 0.1cm of ex] {Understanding-oriented};
			\node[above = 2cm of app] (how) {How-to-guides};
			\node[orange, below = 0.1cm of how] {Task-oriented};
			\node[above = 2cm of acq] (tut) {Tutorials};
			\node[orange, below = 0.1cm of tut] {Learning-oriented};
		\end{tikzpicture}
	\end{center}
	\caption{\diatx\space Foundations \cite{procidaDiataxisDocumentationFramework2023}}\label{fig:diataxis-foundation}
\end{figure}

\noindent
As can be seen in \cref{fig:diataxis-foundation}, the documentation author first ought to decide which scale their documentation applies to.
As the target audience may be interested in practical or theoretical knowledge, the documentation author traces the y-axis of \cref{fig:diataxis-foundation} in positive or negative direction.
\citeauthor{procidaDiataxisDocumentationFramework2023} then instructs documentation authors to decide if their target audience is interested in documentation that either serves further study or leads to directly applicable practical knowledge.
Documentation authors should therefore trace the x-axis of \cref{fig:diataxis-foundation} in positive or negative direction, finally arriving at the type of documentation they should compose for the task at hand \cite{procidaPythonDocsCommunity2022}.
\citeauthor{procidaDiataxisDocumentationFramework2023} explains the purpose of his four types of documentation as follows:
\begin{description}
	\item[Tutorial] Teaches user in basic competence in a skill by having them follow practical steps.
	\item[How-to guide] Helps users apply their skill to solve actual problems by showing steps.
	\item[Explanation] Helps users understand theoretical concepts about their skill.
	\item[Reference] Gives the user information about a skill without outlining the steps they need to take.
\end{description}
Additionally, \citeauthor{procidaDiataxisDocumentationFramework2023} mentions that these types do not pose hard limits on what documentation can contain. They are rather meant to remind the author about the needs of users.
According to these criteria the guides practical section is based on \citeauthor{procidaDiataxisDocumentationFramework2023}s structure for How-to guides, and its theoretical section is based on his structure for Explanation style documentation.

\subsubsection*{How-to guide}
\citeauthor{procidaDiataxisDocumentationFramework2023} implores that How-to guides need to have practical utility and a clear objective.
They can only serve an already competent user.
Thus, the guide on page \pageref{a:guide} has explanations about the basic usage of 3D Slicer and its tools before instructing the user to follow a segmentation method themselves.
\citeauthor{procidaDiataxisDocumentationFramework2023} requires How-to guides to feature imperative language that does not allow for digression or explanation \cite{procidaPythonDocsCommunity2022,procidaDiataxisDocumentationFramework2023}.

\subsubsection*{Explanation}
Explanation documentation according to \citeauthor{procidaDiataxisDocumentationFramework2023} on the other hand is defined as offering context to users.
It is supposed to establish connections between concepts and deepen the users theoretical understanding of the skill
While also offering the user choices and reasons to do something a particular way, or even explain why something is not recommended \cite{procidaPythonDocsCommunity2022,procidaDiataxisDocumentationFramework2023}.


\subsection{Pictograms}\label{s:b-pictograms}
The guide, which can be seen on page: \pageref{a:guide}, makes extensive use of pictograms to inform the reader about common errors, pitfalls to avoid, warn about resource intensive operations and weather a tool is only available via an add-on.
Inspiration for this kind of visual communication was taken from books published by O'Reilly (O'Reilly Media, Inc., Sebastopol, California, U.S.A.), specifically ``Linux observability with \acrshort{bpf}: advanced programming for performance analysis and networking'' by David Calavera and Lorenzo Fontana \cite{calaveraLinuxObservabilityBPF2019}.
Pictograms are stylized drawings that are supposed to represent familiar objects, ideas or abstract concepts to the reader \cite{dowsePharmacistsAreWords2021}.
In health information documents, they have been proven to enhance readability, attractiveness and visual attention of the target audience \cite{houtsRolePicturesImproving2006,mansoorEffectPictogramsReadability2003}.\\


\pagebreak
\section{Segmentation of images and volumes}\label{s:b-segmentation-basics}
Segmentation of images is a process of delineating structures of interest.
It can also be called contouring or annotation and is typically used to delineate anatomical structures, tumors,
malignancies and various other parts of images.
As it is a requirement for visualization of target structures, like a tumor, or quantification tasks like volume and surface measurement,
segmentation is a well established procedure in medical image computing \cite{pinterPolymorphSegmentationRepresentation2019,slicercommunity3DSlicerImage2022}.

\subsection{AI based}\label{s:b-seg-ai}
% images: https://sci-hub.ru/https://doi.org/10.1016/j.neucom.2019.11.118

\subsubsection{Semantic segmentation}
Semantic segmentation can be described as assigning a label to each pixel of an image.
This is commonly performed via machine learning tools like a \acrfull{svm} or \acrfull{fcn}.
For this to be possible, the list of possible labels must be defined before the segmentation task.
Additionally, semantic segmentation aims to partition an image into mutually exclusive subsets,
where each subset is a meaningful region of the original image \cite{haoBriefSurveySemantic2020,longFullyConvolutionalNetworks2015}.

\subsubsection{Instance segmentation}
Semantic segmentation, like mentioned above, focuses on assigning a label from a predefined list to each pixel in an image.
Instance segmentation, on the other hand, alters this behavior by an important concept:
Differentiating between instances of objects belonging to the same class.
It can thus be defined as:
\begin{displayquote}[\cite{chenMaskLabInstanceSegmentation2018}]
	``\ldots\space the task of finding simultaneous solution to object detection as well as semantic segmentation.''
\end{displayquote}
However, in instance segmentations, not every pixel gets its own label, rather a bounding box is created to delineate an instance of a class.
In comparison to semantic segmentations, instance segmentations are commonly performed by a \acrfull{dcnn} or a \acrfull{cnn} \cite{hafizSurveyInstanceSegmentation2020,chenMaskLabInstanceSegmentation2018}.

\subsubsection{Panoptic segmentation}
Panoptic segmentation combines the benefits of the two methods mentioned above
\cite{kirillovPanopticSegmentation2019}.
Every pixel gets assigned a semantic label and a unique instance identifier,
thus Differentiating between instances of the same class and performing an exact image segmentation
\cite{kirillovPanopticSegmentation2019}.

\subsection{Classical}\label{s:b-seg-techniques}
\begin{displayquote}[\cite{gradyEnergyMinimizationApproach2006}]
	``Fully automatic, completely reliable segmentation in medical images is an unrealistic expectation [\ldots]''
\end{displayquote}
Thus, many clinical and preclinical segmentation workflows still rely on classical segmentation tools.
% according to \cite{yuTechniquesChallengesImage2023}
% better: https://www.v7labs.com/blog/image-segmentation-guide#h4
% https://armanasq.github.io/computer-vision/image-segementation/#1-traditional-methods
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/classicalSegTree.png}}
	\caption{Classical segmentation options \cite{v7labsIntroductionImageSegmentation2021}}\label{fig:classicalSegTree}
\end{figure}
\noindent
While \acrshort{ai} based image segmentation techniques revolve around pattern recognition
via some form of machine learning,
classical image segmentation techniques rely on analysis of the qualities of each pixel and its neighborhood.
These qualities may include properties like color, brightness, contrast and intensity.
There exist a number of classical segmentation tools,
\cref{fig:classicalSegTree} gives an overview over the most commonly applied techniques
\cite{yuTechniquesChallengesImage2023,baliReviewStrategiesTechniques2015}.

\subsubsection{Edge Detection}
Edge detection of grayscale images is defined as the task of finding sharp changes of pixel value intensity.
These sharp changes of intensity indicate edges between objects in images.
\begin{figure}[h]
	\centerline{
		\begin{tikzpicture}[outer sep=0pt,minimum size=1cm]
			% labels
			\node [fill={rgb:black,0;white,9}] at (-1,0)  {A};
			\node [fill={rgb:black,0;white,9}] at (-1,-1) {B};
			% A: boxes
			\node [fill={rgb:black,8;white,1}] at (0,0)  {};
			\node [fill={rgb:black,8;white,1}] at (1,0)  {};
			\node [fill={rgb:black,1;white,6}] at (2,0)  {};
			\node [fill={rgb:black,1;white,6}] at (3,0)  {};
			% B: boxes
			\node [fill={rgb:black,8;white,2}] at (0,-1) {};
			\node [fill={rgb:black,8;white,4}] at (1,-1) {};
			\node [fill={rgb:black,8;white,9}] at (2,-1) {};
			\node [fill={rgb:black,7;white,9}] at (3,-1) {};
		\end{tikzpicture}}
	\caption{Edge detection}\label{fig:edgeDetection}
\end{figure}

\noindent
This entails susceptibility to image noise.
In other words, if the image is rich in background noise, quality of segmentation by edge detection will decline.
Furthermore, if the pixel intensity gradient is less pronounced, edge detection quality is also impacted.
Compare \cref{fig:edgeDetection} row A and B.
Row A has a sharply defined edge and thus will pose no difficulty in edge detection tasks.
Row B however, displays less of a steep gradient and will thus result in less accurate edge detection results.

\subsubsection*{Image intensity function}
\begin{figure}[h]
	\centerline{\includegraphics[scale=0.5]{images/edgeModel.png}}
	\caption{Image intensity functions: step and ramp \cite{gonzalezDigitalImageProcessing2007}}\label{fig:iif}
\end{figure}
\noindent
Conceptually, the image intensity function describes changes in intensity.
Wherever the image intensity transitions from one value to another,
the image intensity function changes slope accordingly as can be seen in \cref{fig:iif}.
Edges are thereby signified by certain shapes of the function, such as steps, ramps and roofs.
Edge detection methods work by analyzing the derivative of the image intensity function and
can be categorized into two types of techniques based on the order of derivation they require \cite{gonzalezDigitalImageProcessing2007}.
\begin{figure}[h]
	\centerline{\includegraphics[scale=0.5]{images/edgeModelOrders.png}}
	\caption{Image intensity function derivatives: first, second and zero crossing \cite{gonzalezDigitalImageProcessing2007}}\label{fig:iifO}
\end{figure}
\noindent

\subsubsection*{First order derivative}
The first derivative of an image intensity function, or horizontal intensity profile,
measures the rate of change of pixel intensity (see: \cref{fig:iifO}).
First order derivative edge detection detect edges by analyzing the intensity profile and looking for rapid changes.
The first derivative can be approximated by using gradient operators like the Sobel operator.
This Sobel operator is composed of $3 \times 3$ convolution kernels,
which are specific to the image axis \cite{gonzalezDigitalImageProcessing2007}.
In other words, firstly there is a horizontal kernel $G_{x}$ which is intended to emphasize changes in
intensity in horizontal direction:
\begin{equation}
	G_{x} =
	\begin{bmatrix}
		-1 & 0 & 1 \\
		-2 & 0 & 2 \\
		-1 & 0 & 1
	\end{bmatrix}
\end{equation}
\noindent
Secondly, the vertical kernel $G_{y}$ is used to highlight changes in intensity in vertical direction:

\begin{equation}
	G_{y} =
	\begin{bmatrix}
		-1 & -2 & -1 \\
		0  & 0  & 0  \\
		1  & 2  & 1
	\end{bmatrix}
\end{equation}

\noindent
To conclude, the edge detection workflow using the Sobel kernels is as follows:
\begin{enumerate}
	\item optionally reduce noise by applying Gaussian blur
	\item apply Sobel kernels in $x$ and $y$ axis
	\item calculate gradient magnitude % G = np.sqrt(Gx**2 + Gy**2)
	\item normalization of the output image
\end{enumerate}

\subsubsection*{Second order derivative}
The second derivative measures the rate of change of the first derivative.
This technique analyzes the second order derivative of an image intensity function, or horizontal intensity profile,
and look for so called ``zero crossings''.
These zero crossings are defined as points where the second order derivative changes signs,
in other words, goes from negative to positive or vice versa.
Zero crossings therefore indicate the presence of edges in the rate of change of image intensity \cite{gonzalezDigitalImageProcessing2007}.
The second derivative can be approximated using the Laplacian operator:
\begin{equation}
	Laplace =
	\begin{bmatrix}
		0 & 1  & 0 \\
		1 & -4 & 1 \\
		0 & 1  & 0
	\end{bmatrix}
\end{equation}

\noindent
To conclude, the edge detection workflow using the Laplacian kernel is as follows:
\begin{enumerate}
	\item optionally reduce noise by applying Gaussian blur
	\item apply Laplacian kernel
	\item normalization of the output image
\end{enumerate}

\subsubsection{Region Growing}
\begin{figure}[h]
	\centerline{\includegraphics[scale=0.5]{images/regionGrowing.png}}
	\caption{Region growing seeds and result \cite{pandyaExploringImageSegmentation2024}}\label{fig:rgSR}
\end{figure}
\noindent
Region growing breaks down images into separate areas by grouping areas by their similarity.
This is done by creating a selection of $n$ seed pixels $S$, choosing some sort of principle of similarity
and then grouping pixels adjacent to seeds according to their similarity score into regions $R$ \cite{jourlinVariousContrastConcepts2016,palReviewImageSegmentation1993}.
See \cref{fig:rgSR} for reference: It shows two times the same single slice of an MRI of the human brain.
On the left image some seeds were defined, indicated by the green crosses, on a lesion
and on the right image a region growing algorithm has put all adjacent pixels with similar intensity into the same group.
To elaborate, the basic steps for a region growing algorithm are:
\begin{enumerate}
	\item Selection of $n$ seeds $S$
	\item Selection of similarity score algorithm
	\item Grouping of adjacent pixels according to their similarity into $n$ regions $R$
	\item Adjust similarity threshold or add seeds until result is satisfactory
	\item Stop if similarity is lower than the defined threshold or all pixels belong to a group
\end{enumerate}
\noindent
The similarity score criteria can be any property of a pixels or group of pixels.
Common criteria include color, intensity or texture \cite{jourlinVariousContrastConcepts2016,palReviewImageSegmentation1993}.
\begin{figure}[h!]
	\begin{subfigure}{0.49\textwidth}
		\centerline{
			\begin{tikzpicture}[outer sep=0pt,minimum size=1cm]
				\node [fill={rgb:black,2;white,8}] at (-1,-2) {}; % border top
				\node [fill={rgb:black,2;white,8}] at (0,-2) {};
				\node [fill={rgb:black,2;white,8}] at (1,-2) {};
				\node [fill={rgb:black,2;white,8}] at (2,-2) {};
				\node [fill={rgb:black,2;white,8}] at (3,-2) {};
				\node [fill={rgb:black,2;white,8}] at (-1,-1) {};
				\node [fill={rgb:black,2;white,8}] at (0,-1) {};
				\node [fill={rgb:black,8;white,4}] at (1,-1) {};
				\node [fill={rgb:black,2;white,8}] at (2,-1) {};
				\node [fill={rgb:black,2;white,8}] at (3,-1) {};
				\node [fill={rgb:black,2;white,8}] at (-1,0) {};
				\node [fill={rgb:black,8;white,4}] at (0,0)  {};
				\node [fill={rgb:black,8;white,1}] at (1,0)  {}; % center
				\node [fill={rgb:black,8;white,4}] at (2,0)  {};
				\node [fill={rgb:black,2;white,8}] at (3,0)  {};
				\node [fill={rgb:black,2;white,8}] at (-1,1) {};
				\node [fill={rgb:black,2;white,8}] at (0,1)  {};
				\node [fill={rgb:black,8;white,4}] at (1,1)  {};
				\node [fill={rgb:black,2;white,8}] at (2,1)  {};
				\node [fill={rgb:black,2;white,8}] at (3,1)  {};
				\node [fill={rgb:black,2;white,8}] at (-1,2) {}; % border bottom
				\node [fill={rgb:black,2;white,8}] at (0,2)  {};
				\node [fill={rgb:black,2;white,8}] at (1,2)  {};
				\node [fill={rgb:black,2;white,8}] at (2,2)  {};
				\node [fill={rgb:black,2;white,8}] at (3,2)  {};
			\end{tikzpicture}}
		\caption{4-connected neighborhood}\label{fig:4n}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centerline{
			\begin{tikzpicture}[outer sep=0pt,minimum size=1cm]
				\node [fill={rgb:black,2;white,8}] at (-1,-2) {}; % border top
				\node [fill={rgb:black,2;white,8}] at (0,-2) {};
				\node [fill={rgb:black,2;white,8}] at (1,-2) {};
				\node [fill={rgb:black,2;white,8}] at (2,-2) {};
				\node [fill={rgb:black,2;white,8}] at (3,-2) {};
				\node [fill={rgb:black,2;white,8}] at (-1,-1) {};
				\node [fill={rgb:black,8;white,4}] at (0,-1) {};
				\node [fill={rgb:black,8;white,4}] at (1,-1) {};
				\node [fill={rgb:black,8;white,4}] at (2,-1) {};
				\node [fill={rgb:black,2;white,8}] at (3,-1) {};
				\node [fill={rgb:black,2;white,8}] at (-1,0)  {};
				\node [fill={rgb:black,8;white,4}] at (0,0)  {};
				\node [fill={rgb:black,8;white,1}] at (1,0)  {}; % center
				\node [fill={rgb:black,8;white,4}] at (2,0)  {};
				\node [fill={rgb:black,2;white,8}] at (3,0)  {};
				\node [fill={rgb:black,2;white,8}] at (-1,1)  {};
				\node [fill={rgb:black,8;white,4}] at (0,1)  {};
				\node [fill={rgb:black,8;white,4}] at (1,1)  {};
				\node [fill={rgb:black,8;white,4}] at (2,1)  {};
				\node [fill={rgb:black,2;white,8}] at (3,1)  {};
				\node [fill={rgb:black,2;white,8}] at (-1,2)  {}; % border bottom
				\node [fill={rgb:black,2;white,8}] at (0,2)  {};
				\node [fill={rgb:black,2;white,8}] at (1,2)  {};
				\node [fill={rgb:black,2;white,8}] at (2,2)  {};
				\node [fill={rgb:black,2;white,8}] at (3,2)  {};
			\end{tikzpicture}}
		\caption{8-connected neighborhood}\label{fig:8n}
	\end{subfigure}
	\caption{4- and 8-connected neighborhoods}\label{fig:neighborhoods}
\end{figure}

\noindent
Additionally, a region growing algorithms can be configured to consider a seeds 4-connected neighborhood
(\cref{fig:4n}) or 8 connected neighborhood (\cref{fig:8n}) for similar pixels \cite{jourlinVariousContrastConcepts2016,palReviewImageSegmentation1993}.
Where the black center pixel represents a seed and its grey neighborhood represent the considered pixels.
\pagebreak

\subsubsection{Threshold}
Possibly the simplest classical segmentation method is the threshold.
Whereby the user defines a wanted pixel intensity range and every pixel within that range is set to the binary value $1$
and every other pixel is set to the binary value $0$ \cite{sankurSurveyImageThresholding2004}.

\subsubsection{Clustering}
Clustering, specifically, k-means clustering, refers to the division of a set of data into
a specific number of groups.
For this to be applicable, the items in a group need some kind of similarity.
In image segmentation such a similarity may be pixel properties like color or intensity.
The algorithm then assigns all data points in a group according to their proximity to a seed \cite{dhanachandraImageSegmentationUsing2015}.
The basic steps for a k-means clustering algorithm are as follows:
\begin{enumerate}
	\item Initialize $n$ clusters $k$
	\item Distribute $n$ seed points randomly across the image
	\item Assign each data point to the closest seed point (see: \cref{fig:kmeans1})
	\item Compute the center of each cluster and move the seed point there (see: \cref{fig:kmeans2})
	\item Reassign each datapoint to the new closest seed point
	\item Repeat steps $3 - 5$ until it satisfies a tolerance value
\end{enumerate}
\begin{figure}[h!]
	\begin{subfigure}{0.49\textwidth}
		\centerline{
			\includegraphics[scale=0.5]{images/kmeansSeeds.png}}
		\caption{k-means initial seed points \cite{shabalinKmeansClusteringKmeans2024}}\label{fig:kmeans1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centerline{
			\includegraphics[scale=0.5]{images/kmeansClusters.png}}
		\caption{k-means cluster centers \cite{shabalinKmeansClusteringKmeans2024}}\label{fig:kmeans2}
	\end{subfigure}
	\caption{k-means clustering}\label{fig:kmeans}
\end{figure}
\noindent
A schematic overview over the k-means clustering algorithm can be seen in \cref{fig:kmeans}.
\Cref{fig:kmeans1} displays a set of datapoints, represented as black dots and a small number of
cluster seeds, represented as green circles.
The algorithms first tries to group the datapoints according to their proximity to the seed points, represented as red lines.
In \cref{fig:kmeans2} the seed points have been relocated to better fit the dataset and the clusters have changed accordingly.
% https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050915X00123/1-s2.0-S1877050915014143/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCZp4PSrhRxPtnTYn34CUcNrzLCggJxux51v3rIfZG5cAIgJ8grMnffpcN%2B4hBZ%2BB%2FXW11jaXi0LzjnL3ziudVza0MqsgUIERAFGgwwNTkwMDM1NDY4NjUiDKgtqe05t8VZ17VaQiqPBRIrZOB1Kqo%2Fw0OZhaGsmd9iMC4g%2Fbi%2FayVKxTdeFD0hM4uKsAVsJrFPfPTVG6aWkA5qpnE2OR2QTzfzJ6Rp%2FQaIK5pmiWjq1LkOoHSERw7xofpNLYU3I6pUedtPcxTR4gtKTWvbSzJ3jHfQR72Gt%2BWDxdiTWLMa8wQlqCLsyB4CM0V3eofzi55PvX06Edc8Nz45p17c1Q5o5H2VM%2F4LovT1F7RqNA6q5OiAeI1qolsHxfyLkIuaZhOEHB4ye0%2Br9PFBiHHdqrkay6T8wuLt6ApGUxqfRSlnwrdDR2zjkaiutJlAeoixVJ3AZMsMeXMTT8d5KD8fqinUsY7nheDiD2mAe04nAZ2%2BUW5OSx9NsChZWLK5Blvx5me0OBuN%2B77oQQKAuNSXZXoiHHoJ6YJJGnjxS6T0OGnE0dPa45AMa8IZ1Ng4WdIa8PqFVnDupyuNWqjRWXe3GGpNfsJI1N%2Fucbx0HIwICzf1Q8nF6k1qFyvoichsduRgiETKRGtqBlG5Fb1QZDpJWvp4pvRA40heyM71WUcN46NlzGslFy6IPDN5oybVAhMXVzgCTqVwJ%2FiAUX8YTaqGvGyW3CVJ3wwNY9Evf8MXq4%2Fp1ONOiuuVmgeoDiJFusuqr5Q%2FNcgKUbkA1EEvKHuEhl92PmCIzf%2Bm7Y9jH3Z6oKUv9xIX9R2AjAEA0ehQQX9jzs2f59wbJoVttOYOrNLiwDwqWaIb40yobzIDUAQdyuuwWn3I5JlNPbm4dfZSXWgQuSfZz5Mrd6QNNZ55griE78kZIxxRZsngCabXiE%2FZZIGyhjK5bK358ZE8eqel91R%2FmmUJHgYVN%2BaDVx9Pf4DwzQN%2Bx0bV8c%2ByRC2hQ81th3tugGu0LhRMoTQwpPWIuAY6sQEeT%2Fz6yzL%2BWOGhM4Aewq8w%2Fy7B7cX4bAoffQLTJjAnuXMSQq%2BehSYxT47YL3%2FQ4ne5VH%2FVAeGrCvzZIkLjkn4VbYpGdEQOMscq6ZjxRnv%2BliSNb57yBFuIc0WCxZZdIYPTDv%2F%2BSTQfighgKyd20vQ9n6hu%2BBYI4yIaOVkATQcCILSydjvkV4kj4pP6DYfqkgyQcC5VBHJbx8OKqwsL58Lb1zIvfjuEEvD1vKOG2fp0JYg%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241006T082633Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYZUW2AN72%2F20241006%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8d4fcc169a59911c17b33d41de99111eacec7a0c5d7c687fc63b6849d16e1b53&hash=a8d5ddfcc08af5477e7f38b52d1fce0618e897ed9b746ca6f3677236ea96ee9d&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877050915014143&tid=spdf-bddd0a9c-0fa1-44da-80dd-d9f07327e225&sid=cc00750590fd924ecb6be7a3418808ba2584gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1b125f020752545a565257&rr=8ce4446698c85a71&cc=at
% https://towardsdatascience.com/image-color-segmentation-by-k-means-clustering-algorithm-5792e563f26e
% https://www.kdnuggets.com/2019/08/introduction-image-segmentation-k-means-clustering.html


\subsubsection{Watershed}
% https://medium.com/codex/watershed-segmentation-locate-individual-objects-when-heavily-clustered-b07bb1dbc0f6
% https://sci-hub.ru/https://doi.org/10.1016/B978-0-12-415873-3.00004-3
The watershed algorithm treats pixel intensities as elevations in a virtual landscape.
In this landscape, local minimal are seen as the seed for ``catchment basins''.
On the other hand, local maxima, or so-called ``watersheds'' separate the basins from each other (see: \cref{fig:water1}).
By ``filling the basins with water'' each pixel in an image is either assigned to a region or a watershed.
Finally, by modifying the water height, which in practice equates to a threshold value,
the watershed algorithm can be adjusted to either create larger or smaller regions (see: \cref{fig:water2}).
If the merging of two regions is to be avoided, an additional watershed, called ``dams'',
can be placed between them \cite{preimImageAnalysisMedical2014,hahnIWTinteractiveWatershedTransform2003}.
\begin{figure}[h!]
	\begin{subfigure}{0.49\textwidth}
		\centerline{
			\includegraphics[scale=0.5]{images/watershed1.png}}
		\caption{Watershed minima with basins \cite{hahnIWTinteractiveWatershedTransform2003}}\label{fig:water1}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centerline{
			\includegraphics[scale=0.5]{images/watershed2.png}}
		\caption{Watershed dams and flooding level \cite{hahnIWTinteractiveWatershedTransform2003}}\label{fig:water2}
	\end{subfigure}
	\caption{Watershed landscape}\label{fig:watershed}
\end{figure}


\section{Segmentation representation}
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/segRep.png}}
	\caption{Brain stem in different representations: A: Binary labelmap, B: Closed surface, C: Fractional labelmap, D: Planar contours, E: Ribbon mode \cite{pinterPolymorphSegmentationRepresentation2019}}\label{fig:segRep}
\end{figure}
\noindent
However, segmentations can be represented in a number of data structures.
\Cref{fig:segRep} gives an overview of the most common representations, of those shown, the binary labelmap is the most commonly used storage format.
For most workflows, a single representation of data is not sufficient.
An example for the requirement of several data structures within a single workflow is the calculation of \acrfull{dvh}s in radiation therapy.
The \acrfull{ptv} and \acrfull{oar} are typically segmented by defining their cross-section per
image slice on a planning CT or MRI for the extent of the whole structure \cite{burnetDefiningTumourTarget2004}.
This results in a list of planar contours similar to \cref{fig:segRep}/D.
These planar contours must then be voxelized into binary labelmaps, like in \cref{fig:segRep}/A.
As for the next step, these binary labelmaps are used to compute dose distribution volumes and \acrlong{dvh}.
If a workflow requires 3D visualization, closed surfaces must be constructed, this can be done by converting,
using either planar contours or binary labelmaps as the source dataset \cite{pinterPolymorphSegmentationRepresentation2019}.\\
The following sections should give a short overview over the mentioned data structures and their use cases.

% https://pdf.sciencedirectassets.com/276826/1-s2.0-S1002007108X00061/1-s2.0-S1002007108000658/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIC0651E8f8wYnatuSWoH5B6QUXpJl5MbGRvWCkKGH3eIAiBvnyYEsTNrvIQIGrGVLvmFluaWtP7WINLlnbDCpX46xSq8BQi5%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIM1STppGJCBYoUEBeVKpAFWAcGFyUcG0btJR%2FneE3O%2BBlTxy7E5ofkZHmqUQwnBVCsLpchDXvIfS1JRiLoVLZiyut6RbzSQ1yXcHQVTGvXSkrIXaFJLUgzlOV2P5cA0z9otXoyMxNKJe9b3M5%2Bri6HwsjS2tecJoVGp9I%2Fxt5Vto%2B%2BW9l6a%2B6mikvKuxRzl1A5ymZbAXBeweI3NOXYN%2F0mGMzxLCe5LZKPlleOf6PhI%2B%2Fn38ROevhlXMzKa7lBUNQrmxUY5PItKNCVCqpGOx8iaHxVwbA%2BvqDdjIgE1gWp57P4Ws0QQQWzjEFPkr%2FQtc%2F5QW5zYun9A59QfK0rzVYYifEXXwILRVPdIJnObDY0t8zkqON%2F%2FbETulttNw7QtMG%2FyXxPkP2cbZXFPg28T%2BdBEAXQpPR7aRqyhZwCS5%2FiO%2BFvdO0U59U9bJFKWxbf%2BrRbCzSCCjIQhbu85L8Iaw7mW9KqFlDbxjFsjn1gofo7YAaG7FkBZrto%2BtdicJU3VQu01UrUMXuYDHr6iuFiwpBVIspyaTnq0xkBMz8uD4mniBMfTszP%2FGpobudWY2PT9Aiq2peJN%2FKSr%2FqfpA3iQoJIuTxvlAlEXTDmQSUW7JLuQWakWZagmARMJM8BMPMJZyGRd1yvW%2Ba97uUq8C874DWzhJtC3bnHl1bs0sCt5kXy9wgR%2Fc7juoY6ww76MjVPj9PRXXSsNPIgm64srDQIpk791E8PX%2B2osKa3HTxi82IrSaBjFdd3%2F0t%2FqbUol5MlvZCDiQDuJZuMIIE12ZNf9zViJOAc2twSKTH6esVAD8tquVviw9Y16yfmo6E56J%2BBvPuDqD9xLSkVsKbMIngZtGjvxZr9NYWUuDzs3yZqLnzWD7N9Zx2xQvgkgrqLqRLSTo0w18TEtwY6sgFg2sYrBVEID6iQHkMLCCHBShdm0l0WNHPr%2FVBXRSU2aHYR8wp7pP9IjKVw3IPSSCCz4i2Mbc%2BYSpTvjEch5dhv5%2FyVMG%2Fp0hIpijQTajDlXOCIb4hiEzk5L2ar77cZfxUSNJfCM7srS7gCzG0K5ZYZAEf29r16H85UcDMyejTA1FvEQGJ%2B3KnYr1gy0DzAy7gB5C%2F%2Fjf7gh%2BvbMFv0Xtv%2FKvrCWMnV2Jzyyu6ndkg%2BHlZU&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240923T084001Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYV3XLASKH%2F20240923%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e8cf422cc462ca2f9cc477cbdea04e2e7c981543eec928e444da7d58b9d58564&hash=297d50f9d1f50c00ae81b7c033038ede324643f9d36bd7eb5099a63fd99c4816&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1002007108000658&tid=spdf-f0276a20-ff46-4f00-a9af-e2bccba738f8&sid=b1fc4ae88be0f942df7a2d8859741e612e47gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1b125f02555f530f530257&rr=8c793a3f998ac2aa&cc=at
% https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0068449&type=printable
% https://sci-hub.st/10.1007/978-3-030-16641-0


\subsection{Binary labelmap}\label{s:b-binary-labelmap}
Binary labelmaps (see: \cref{fig:segRep}/A) are the most common representation as they have several benefits for segmentation software developers.
They are comparatively simple to edit, as changes to the labelmap translate to addition or subtraction of voxels to the segment label.
Storage is simplified as well, because each voxel can only belong to a single segment, this enables storage of the whole dataset in a single 3D array.
If label overlap is desired, the base 3D volume needs to be duplicated,
in order to store the overlapping segment in a separate 3D array.
This in turn inflates storage requirements for the segmentation,
thus binary labelmaps tend to be memory inefficient for overlapping structures \cite{slicercommunity3DSlicerImage2022}.
\begin{figure}[h]
	\begin{minted}{text}
label, X, Y, Z
[...]
1, 0.23563218390804597, 0.13218390804597702, 0.27586206896551724
0, 0.09051724137931035, 0.28304597701149425, 0.1810344827586207
5, 0.03879310344827586, 0.27873563218390807, 0.07758620689655173
5, 0.1896551724137931, 0.40804597701149425, 0.16666666666666666
2, 0.1997126436781609, 0.5014367816091954, 0.1781609195402299
[...]
\end{minted}
	\caption{sample binary labelmap 3D array}\label{fig:binary-array}
\end{figure}

\noindent
\Cref{fig:binary-array} displays a sample of such a 3D array.
The first column refers to segment labels as decimal numbers,
which in turn means that the alphanumeric label must be stored in a separate \acrfull{lut}.
The remaining three columns are used to store the voxel location in 3D space.
Meaning, each entry in the binary labelmap 3D array corrensponds to a specific voxel in the source dataset.
As a result, binary labelmap segmentation resolution is tied to the source dataset resolution \cite{pinterPolymorphSegmentationRepresentation2019,sunderlandFractionalLabelmapsComputing2017,weber3DSlicerPlatform2022}.


\subsection{Fractional labelmap}\label{s:b-fractional-labelmap}
Fractional labelmaps (see: \cref{fig:segRep}/C) are a segmentation representation with the goal of improving on the concept of binary labelmaps mentioned above.
Instead of a single voxel value representing a discrete label, in fractional labelmaps,
each voxel value represents the probability that a structure is contained inside or outside that voxel.
As a result, fractional labelmaps can store higher segmentation resolutions than the source dataset
\cite{warfieldFractionalSegmentationWhite1999,noePartialVolumeSegmentation2001,pinterPolymorphSegmentationRepresentation2019,sunderlandFractionalLabelmapsComputing2017,weber3DSlicerPlatform2022}.


\subsection{Closed surface}\label{s:b-closed-surface}
Closed surface representations like in \cref{fig:segRep}/B are composed of a triangulated mesh,
which in turn is composed of a series of vertices or triangles.
In terms of segmentation, the mesh surface boundary defines the segmentation volume,
so that every voxel encompassed by the mesh belongs to the segment \cite{delameMedialSurfaceMesh2012,weber3DSlicerPlatform2022,pinterPolymorphSegmentationRepresentation2019}.
In order to store a vertex representation, each triangle (vertex) needs to be stored separately.
A schema of the representation can be seen in \cref{fig:surfacemesh-schema}.
The representation of a single voxel starts with the definition of its facet normal or unit normal vector in 3D cartesian space.
This vector points outwards from the vertex, effectively defining the basic orientation of the vertex.
Then, the triangle corners are defined in 3D cartesian space ($x,y,z$).
\Cref{fig:surfacemesh} gives an overview of a possible triangle orientation.
Here it can be seen that, the first corner intersects with the third corner on the position
$x=-1$ and $z=1$.
The second corner intersects with the first one only on position $z=1$ but with the third corner on positions
$y=1$ and $z=1$.
Finally, the third corner intersects with the first corner on position $x=-1$ and
with the second corner on positions $y=1$ and $z=1$.

\begin{figure}[h] % taken from cube stl file
	\begin{subfigure}{0.49\textwidth}
		\begin{verbatim}
			facet normal n(i) n(j) n(k)
			[...]
			  vertex v1(x) v1(y) v1(z)
			  vertex v2(x) v2(y) v2(z)
			  vertex v3(x) v3(y) v3(z)
			[...]
		\end{verbatim}
		\caption{Closed surface mesh schema}\label{fig:surfacemesh-schema}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\begin{verbatim}
			facet normal 0.000000 0.000000 1.000000
			[...]
			  vertex -1.000000 -1.000000 1.000000
			  vertex 1.000000 1.000000 1.000000
			  vertex -1.000000 1.000000 1.000000
			[...]
		\end{verbatim}
		\caption{Closed surface mesh sample}\label{fig:surfacemesh}
	\end{subfigure}
	\caption{surface mesh representation}\label{fig:mesh-repesentation}
\end{figure}

\noindent
To finally form a segmentation form these vertices, a series of them must be composed that encompasses the whole segmentation volume.
This in turn results in a representation like in \cref{fig:spineMesh}.
% https://sci-hub.ru/https://doi.org/10.1007/s11548-015-1320-0
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4447523/
% https://link-springer-com.wn.idm.oclc.org/article/10.1007/s10439-017-1799-3#citeas
% https://elib.dlr.de/187800/1/Weber_Jonas_BSc_Thesis.pdf
% https://sci-hub.ru/https://doi.org/10.1007/978-3-540-88693-8_43
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.4]{images/spineMesh.png}}
	\caption{Closed surface mesh of a mouse vertebrae segmentation}\label{fig:spineMesh}
\end{figure}

\noindent
The closed surface representation is ideal for 3D visualization of segmentations but has some downsides as well.
First and foremost, they can be self intersecting and thus be invalid representations of reality.
Furthermore, to edit a closed surface, point coordinates of individual vertices must be modified
and the unit normal vector recalculated \cite{pinterPolymorphSegmentationRepresentation2019}.


\subsection{Planar contours and ribbons}\label{s:b-planar-contours}
Planar contours (see: \cref{fig:segRep}/D) as well as ribbon models (see: \cref{fig:segRep}/E) define a segmentation,
by encompassing it in rings or ribbons for each slice separately.
They are created by tracing contours around a \acrfull{roi} in parallel planar slices \cite{pinterPolymorphSegmentationRepresentation2019,weinsteinScanlineSurfacingBuilding2000}.
In 2D planar contours are represented by polylines.
In 3D representations however, these polylines need to be stacked, gaps require interpolation and the final volume is then smoothed.

\begin{figure}[h]
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[scale=0.5]{images/planarContours.png}
		\caption{Planar contours outlining structures of interest \cite{weinsteinScanlineSurfacingBuilding2000}}\label{fig:planarContoursLines}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[scale=0.5]{images/planarContoursRecon.png}
		\caption{Reconstructed surfaces from planar contours \cite{weinsteinScanlineSurfacingBuilding2000}}\label{fig:planarRec}
	\end{subfigure}
	\caption{Planar contour representation}\label{fig:planarContours}
\end{figure}

\noindent
As can be seen in \cref{fig:planarContoursLines} planar contours or ribbons can be highly accurate in 2D.
Experts can also draw polylines regardless of noise, low-resolution data, and overlapping pathologies.
Furthermore, for radiation therapy, the \gls{dicom} standard requires segmentations to be stored as ``structured sets'',
essentially a series of planar contours.
However, when it comes to 3D visualization, planar contours as well as ribbons offer comparatively poor quality visuals \cite{pinterPolymorphSegmentationRepresentation2019,weinsteinScanlineSurfacingBuilding2000}.


\section{Mouse bone anatomy}\label{s:b-mouse-anatomy}
This section is dedicated to bone anatomy of laboratory mice (Mus musculus) (see: \cref{fig:mouseFull}).
Apart from the obvious size difference, mice are biologically very comparable to humans (Homo sapiens),
notable differences between human and mouse bone anatomy will be pointed out \cite{ruberteBridgingMouseHuman2023}.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.5]{images/mouseFull.png}}
	\caption{Mouse bone model \cite{ruberteBridgingMouseHuman2023}}\label{fig:mouseFull}
\end{figure}

%book \cite{ruberteMorphologicalMousePhenotyping2017}\\
%open anatomy \cite{halleOpenAnatomyBrowser2017}\\
%spine paper \cite{harrisonVertebralLandmarksIdentification2013}
%comparison \cite{ruberteBridgingMouseHuman2023}
%comp2 \cite{jeromeSkeletalSystem2018}
% humans \cite{platzerTaschenatlasAnatomieBd2013}


\subsection{Skull}\label{s:b-mouse-skull}
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.7]{images/mouseSkull.png}}
	\caption{Mouse skull \cite{ruberteBridgingMouseHuman2023}}\label{fig:mouseSkull}
\end{figure}

\noindent
Most notably, the mouse skull reflects the importance of its sense of smell (olfaction),
by its elongated facial features and large tooth size (see: \cref{fig:mouseSkull}).
For a visual representation see: \cref{fig:mouseSkull}.
Meanwhile, the human head is shaped for a larger brain size.
In mice the interparietal bone is well-developed when compared to humans.
Furthermore, in mice the two mandibles are not fused together like in humans,
they are rather joined by cartilaginous tissue \cite{jeromeSkeletalSystem2018,halleOpenAnatomyBrowser2017,platzerTaschenatlasAnatomieBd2013}.

\subsection{Spine}
\label{s:b-mouse-spine}
Mouse spines (see: \cref{fig:mouseSpine}) are made of 7 cervical vertebrae, 13 thoracic vertebrae, 6 lumbar vertebrae, 4 sacral vertebrae and 20-30 caudal vertebrae \cite{platzerTaschenatlasAnatomieBd2013}.
Mice can possess up to 60 vertebrae, whereas humans only possess 33\footnote{34 with anatomical normal variant L6}.
The largest part of this difference is made up by the fact that mice possess a tail composed of 27-31 coccygeal vertebrae,
while humans only possess 4 non-articulated coccygeal vertebrae.
The remainder is due to minor differences in thoracic, lumbar and sacral vertebrae counts \cite{ruberteMorphologicalMousePhenotyping2017,jeromeSkeletalSystem2018,harrisonVertebralLandmarksIdentification2013}.
Rodent and human vertebrae additionally differ in vertebral body size.
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.7]{images/mouseSpine.png}}
	\caption{Mouse spine X-ray and \acrshort{mri} \cite{harrisonVertebralLandmarksIdentification2013}}\label{fig:mouseSpine}
\end{figure}

\noindent
Rodent vertebrae have relative a comparatively small body as due to their quadrupedal nature,
mice spines do not undergo as much axial stress as human spines \cite{ruberteMorphologicalMousePhenotyping2017,jeromeSkeletalSystem2018,platzerTaschenatlasAnatomieBd2013}.

\subsection{Thorax and Pelvis}\label{s:b-mouse-th-pel}
\begin{figure}[h]
	\centerline{
		\includegraphics[scale=0.6]{images/mouseThorax.png}}
	\caption{Mouse thorax X-ray \cite{harrisonVertebralLandmarksIdentification2013}}\label{fig:mouseThorax}
\end{figure}

\noindent
In many quadrupedal mammals the clavicle has dramatically reduced or even disappeared.
Despite their quadrupedal nature mice have a well-developed clavicle,
that joins the sternum and acromion of the scapula like in humans \cite{ruberteBridgingMouseHuman2023}.
Mice possess 13 pairs of ribs (costae) as can be seen in \cref{fig:mouseThorax}, including 7 pairs that are articulated to the sternum, also called true ribs
and 3 pairs of asternal or false ribs.
This is very similar to human anatomy, with the most notable difference being that humans only have 12 pairs of ribs.
The mouse pelvis, like in humans is composed of the ilium, ischium and pubis \cite{platzerTaschenatlasAnatomieBd2013,ruberteMorphologicalMousePhenotyping2017,halleOpenAnatomyBrowser2017}.


\subsection{Extremities}
\label{s:b-mouse-ext}
\begin{figure}[h]
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[scale=0.5]{images/mouseFrontPaw.png}
		\caption{Mouse front paw \cite{ruberteMorphologicalMousePhenotyping2017}}\label{fig:frontPaw}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[scale=0.5]{images/mouseHindPaw.png}
		\caption{Mouse hind paw \cite{ruberteMorphologicalMousePhenotyping2017}}\label{fig:hindPaw}
	\end{subfigure}
	\caption{Mouse paw anatomy}\label{fig:mousePaws}
\end{figure}

\noindent
As can be seen in \cref{fig:frontPaw} after the mouse radius and ulna, the wrist joint is made up of 9 bones:\\
The carpal bones I-IV, the intermedioradial carpal bone, the ulnar carpal bone, the accessory carpal bone,
the central carpal bone, and the proximal sesamoid bones.
Which then are articulated with 5 metacarpal bones and phalanges like in humans.
Humans however posses 8 carpal bones, which include the scaphoid, lunate, triquetrum, pisiform,
trapezium, trapezoid, capitate and hamate \cite{platzerTaschenatlasAnatomieBd2013}.

\noindent
As can be seen in \cref{fig:hindPaw}, the mouse ankle is made up of 8 bones:
The talus, calcaneus, tibial tarsal bone, central tarsal bone and tarsal bones I-IV.
Whereas the human ankle joint is made up of 7 bones:
Calcaneus, cuboid, talus, navicular, lateral cuneiform, middle cuneiform, medial cuneiform \cite{platzerTaschenatlasAnatomieBd2013}.

\noindent
Supratrochlear foramen in humerus in mice but not in humans \cite{ruberteMorphologicalMousePhenotyping2017,platzerTaschenatlasAnatomieBd2013}
radius and ulna are independent and not fused as it occurs in other mammals

\noindent
The rodent fibula is half the length of those of humans and fuses with the tibia,
whereas the human fibula articulates with the ankle \cite{jeromeSkeletalSystem2018,platzerTaschenatlasAnatomieBd2013}.

% https://sci-hub.ru/https://doi.org/10.1016/B978-0-12-802900-8.00005-1
